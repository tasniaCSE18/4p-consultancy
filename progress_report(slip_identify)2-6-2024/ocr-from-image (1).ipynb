{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8503164,"sourceType":"datasetVersion","datasetId":5074902},{"sourceId":8529190,"sourceType":"datasetVersion","datasetId":5093673},{"sourceId":8529198,"sourceType":"datasetVersion","datasetId":5093681},{"sourceId":8545576,"sourceType":"datasetVersion","datasetId":5105696},{"sourceId":8548508,"sourceType":"datasetVersion","datasetId":5107868},{"sourceId":8582486,"sourceType":"datasetVersion","datasetId":5132761},{"sourceId":8599668,"sourceType":"datasetVersion","datasetId":5145013}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n Input data files are available in the read-only \"../input/\" directory\n For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"pip install googletrans pandas openpyxl\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:17:07.762958Z","iopub.execute_input":"2024-06-02T03:17:07.763693Z","iopub.status.idle":"2024-06-02T03:17:25.999084Z","shell.execute_reply.started":"2024-06-02T03:17:07.763660Z","shell.execute_reply":"2024-06-02T03:17:25.997737Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting googletrans\n  Downloading googletrans-3.0.0.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\nCollecting httpx==0.13.3 (from googletrans)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2024.2.2)\nCollecting hstspreload (from httpx==0.13.3->googletrans)\n  Downloading hstspreload-2024.6.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.3.0)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=9593c4bc9bf6c34a8f9927980859b8d6d7f0201623709369acbcb87108cc74bb\n  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.5\n    Uninstalling httpcore-1.0.5:\n      Successfully uninstalled httpcore-1.0.5\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Uninstalling httpx-0.27.0:\n      Successfully uninstalled httpx-0.27.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\njupyterlab 4.1.6 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom googletrans import Translator\nimport time\n\n# Load the Excel file\nfile_path = '/kaggle/input/ocrtext/Ocr500 (2).xlsx'\ndata = pd.read_excel(file_path)\n\n# Initialize the translator\ntranslator = Translator()\n\n# Function to translate text to English with retry logic and logging\ndef translate_to_english(text):\n    try:\n        if not pd.isnull(text) and isinstance(text, str) and text.strip() != '':\n            # Attempt to translate with a retry mechanism\n            attempts = 3\n            for attempt in range(attempts):\n                try:\n                    translated = translator.translate(text, dest='en')\n                    return translated.text\n                except Exception as e:\n                    if attempt < attempts - 1:\n                        time.sleep(1)  # Wait before retrying\n                    else:\n                        return f\"Translation error after {attempts} attempts: {str(e)}\"\n        else:\n            return text\n    except Exception as e:\n        return f\"Error in translation function: {str(e)}\"\n\n# Apply the translation function to the 'Extracted Text' column\ndata['translated_text'] = data['Extracted Text'].apply(translate_to_english)\n\n# Save the results to a new Excel file\noutput_file_path = '/kaggle/working/Ocr500_translated.xlsx'\ndata.to_excel(output_file_path, index=False)\n\n# Display the translated text data\ndata[['Image Name', 'Extracted Text', 'translated_text']].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T03:17:32.653438Z","iopub.execute_input":"2024-06-02T03:17:32.653842Z","iopub.status.idle":"2024-06-02T03:39:44.687564Z","shell.execute_reply.started":"2024-06-02T03:17:32.653806Z","shell.execute_reply":"2024-06-02T03:39:44.686700Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"       Image Name                                     Extracted Text  \\\n0  PRS208C4002621  শীতল ক্যান্টনমেন্ট ডায়াগনস্টিক সেন্টার, রংপুর...   \n1  PRS208C4017521  IBN SINA\\nIBN SINA MEDICAL COLLEGE HOSPITAL\\n1...   \n2  PRS208C4018544  অধ্যাপক ডাঃ শেখ নুরুল ফাত্তাহ রুমি\\nএমবিবিএস (...   \n3  PRS208C4018582  অধ্যাপক ডাঃ মওদুদুল হক\\nএমবিবিএস, এমডি, পিএইচড...   \n4  PRS208C4018608  অধ্যাপক ডাঃ বেগম হোসনে আরা\\nএমবিবিএস, এফসিপিএস...   \n\n                                     translated_text  \n0  Translation error after 3 attempts: 'NoneType'...  \n1  Translation error after 3 attempts: 'NoneType'...  \n2  Translation error after 3 attempts: 'NoneType'...  \n3  Translation error after 3 attempts: 'NoneType'...  \n4  Translation error after 3 attempts: 'NoneType'...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image Name</th>\n      <th>Extracted Text</th>\n      <th>translated_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PRS208C4002621</td>\n      <td>শীতল ক্যান্টনমেন্ট ডায়াগনস্টিক সেন্টার, রংপুর...</td>\n      <td>Translation error after 3 attempts: 'NoneType'...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PRS208C4017521</td>\n      <td>IBN SINA\\nIBN SINA MEDICAL COLLEGE HOSPITAL\\n1...</td>\n      <td>Translation error after 3 attempts: 'NoneType'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PRS208C4018544</td>\n      <td>অধ্যাপক ডাঃ শেখ নুরুল ফাত্তাহ রুমি\\nএমবিবিএস (...</td>\n      <td>Translation error after 3 attempts: 'NoneType'...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PRS208C4018582</td>\n      <td>অধ্যাপক ডাঃ মওদুদুল হক\\nএমবিবিএস, এমডি, পিএইচড...</td>\n      <td>Translation error after 3 attempts: 'NoneType'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PRS208C4018608</td>\n      <td>অধ্যাপক ডাঃ বেগম হোসনে আরা\\nএমবিবিএস, এফসিপিএস...</td>\n      <td>Translation error after 3 attempts: 'NoneType'...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport shutil\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor filename in files:\n    # Construct file path\n    file_path = os.path.join(source_dir, filename)\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Append filename and extracted text to lists\n    filenames.append(filename)\n    extracted_texts.append(extracted_text)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T04:15:57.100395Z","iopub.execute_input":"2024-05-31T04:15:57.100827Z","iopub.status.idle":"2024-05-31T04:19:04.746014Z","shell.execute_reply.started":"2024-05-31T04:15:57.100802Z","shell.execute_reply":"2024-05-31T04:19:04.744900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Excel file\nfile_path = '/kaggle/input/translated/Ocr500_transliterated4.xlsx'\nexcel_data = pd.ExcelFile(file_path)\n\n# Load the sheets into DataFrames\nsheet1 = pd.read_excel(excel_data, sheet_name='Sheet1')\nsheet3 = pd.read_excel(excel_data, sheet_name='Sheet3')\n\n# List of new Row Labels to be matched (assuming they are provided separately)\nnew_row_labels = [\"বহির্বিভাগীয় রোগীর টিকিট\",\"OPD/EMERGENCY TICKET\",\"OPD /EMERGENCY TICKET\", \"OPD / EMERGENCY TICKET\",\"বহির্বিভাগীয় রোগীর টিকেট\",\"জরুরী বিভাগ রোগীর টিকিট\",\n                   \"বহিবিভাগীয় রোগীর টিকেট\",\"জরুরী বিভাগ\",\n                  \"বহিঃ বিভাগের রোগীর টিকিট\",\n                  \"বহিঃ বিভাগ টিকিট\",  \"বহির্বিভাগ রোগীর টিকিট\",\"বহিঃ বিভাগ চিকিট\",  \"বহিঃ বিভাগ\",\n                \"বহির্বিভাগের রোগীর টিকিট\", \"OPD PRESCRIPTION\",  \"বাংলাদেশ ফরম নং ৭৬৯\",\"OPD ROOM\",\"বহির্বিভাগের রোগীর টিকেট\", \"বহির্বিভাগ\",\"ব্যবস্থাপত্র\",\n                        \"OPD TICKET\",\"Outpatient Ticket\",\"বহি বিভাগ রোগীর টিকেট\",\n               \"রোগী ভর্তির ফরম ও রোগ বৃত্তান্ত\",\"রোগী ভর্তির ফরম ও রোগ বৃত্তাত্ত\",\"রোগী ভর্তির ফরম\",\"OUTDOOR CHECKUP TICKET\",\n                 \"বহিরবিভাগের রোগীর টিকেট\",\n                   \"Registration Card\",    \"বহিঃবিভাগীয় রোগীর টিকিট\",  \"জরুরী\" ,  \"বর্হিবিভাগ রোগীর টিকিট\",\n                  \"বর্হিবিভাগীয় রোগীর টিকিট\", \"বহি বিভাগ\",  \"বিভাগের রোগীর টিকিট\",  \"ৰৰ্হি বিভাগীয় রোগীর টিকিট\"\n]\n\n# Function to match Row Labels with Extracted Text\ndef match_row_labels(extracted_text, labels):\n    for label in labels:\n        if label in extracted_text:\n            return label\n    return None\n\n# Apply the matching function to create SLIP_Identifier column\nsheet1['SLIP_Identifier'] = sheet1['Extracted Text'].apply(lambda x: match_row_labels(x, new_row_labels))\n\n# Write 'slip' in Type1 column if SLIP_Identifier is found\nsheet1['Type1'] = sheet1['SLIP_Identifier'].apply(lambda x: 'slip' if pd.notna(x) else None)\n\n# Save the updated sheet back to a new Excel file\noutput_file_path = '/kaggle/working/path_to_your_updated_file_with_type1.xlsx'\nwith pd.ExcelWriter(output_file_path) as writer:\n    sheet1.to_excel(writer, sheet_name='Sheet1', index=False)\n    sheet3.to_excel(writer, sheet_name='Sheet3', index=False)\n\nprint(f\"Updated file saved at {output_file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:37:02.574229Z","iopub.execute_input":"2024-06-02T10:37:02.574707Z","iopub.status.idle":"2024-06-02T10:37:02.903594Z","shell.execute_reply.started":"2024-06-02T10:37:02.574672Z","shell.execute_reply":"2024-06-02T10:37:02.902561Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Updated file saved at /kaggle/working/path_to_your_updated_file_with_type1.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\n\nfile_path = '/kaggle/input/physician-name/M05D28T17Alamin_Missing1.xlsx'  # Replace with the actual path to your Excel file\ndata = pd.read_excel(file_path)\n\ndef extract_keyword_and_two_words(text):\n    # Look for 'Dr' or 'ডাঃ' and extract the keyword and the two words following these keywords\n    match = re.search(r'(Dr|ডা)\\.?([\\S]+) ([\\S]+)', text, re.IGNORECASE)\n    if match:\n        return f\"{match.group(1)} {match.group(2)} {match.group(3)}\"\n    return None \n\n# Apply the function to extract the keyword and two words after the keyword from the 'Extracted Text' column\ndata['doctor_name'] = data['Extracted Text'].apply(extract_keyword_and_two_words)\n\ndata.head()\n\noutput_file_path = '/kaggle/working/extracted_doctor_names.xlsx'  \ndata.to_excel(output_file_path, index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T10:00:18.139719Z","iopub.execute_input":"2024-06-04T10:00:18.140374Z","iopub.status.idle":"2024-06-04T10:00:18.233048Z","shell.execute_reply.started":"2024-06-04T10:00:18.140342Z","shell.execute_reply":"2024-06-04T10:00:18.232288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport pandas as pd\nfrom IPython.display import display\n\n# Ensure Tesseract OCR is installed\n!apt-get update\n!apt-get install -y tesseract-ocr\n!apt-get install -y tesseract-ocr-ben\n!apt-get install -y training-tools\n!pip install pytesseract pillow pandas\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Set the batch size\nbatch_size = 50\nnum_batches = (len(files) + batch_size - 1) // batch_size\n\n# Function to preprocess an image\ndef preprocess_image(image_path, output_path):\n    with Image.open(image_path) as img:\n        # Convert to grayscale\n        img = img.convert(\"L\")\n\n        # Adjust brightness and contrast\n        enhancer = ImageEnhance.Contrast(img)\n        img = enhancer.enhance(2)\n\n        # Reduce noise\n        img = img.filter(ImageFilter.MedianFilter())\n\n        # Sharpen the image\n        img = img.filter(ImageFilter.SHARPEN)\n\n        # Resize the image to one-third its original dimensions\n        original_width, original_height = img.size\n        target_width = original_width // 3\n        target_height = original_height // 3\n        img_resized = img.resize((target_width, target_height), Image.ANTIALIAS)\n\n        # Save the preprocessed image\n        img_resized.save(output_path)\n\n# Create a list of image files and corresponding box files\nimage_files = [os.path.join(source_dir, f) for f in files if f.endswith('.png')]\nbox_files = [f.replace('.png', '.box') for f in image_files]\n\n# Preprocess images and save them\npreprocessed_dir = '/kaggle/input/prescription-image'\nos.makedirs(preprocessed_dir, exist_ok=True)\n\npreprocessed_image_files = []\nfor file in image_files:\n    preprocessed_image_path = os.path.join(preprocessed_dir, os.path.basename(file))\n    preprocess_image(file, preprocessed_image_path)\n    preprocessed_image_files.append(preprocessed_image_path)\n\n# Function to train Tesseract on a set of images and their corresponding box files\ndef train_tesseract(image_files, box_files, output_dir):\n    # Combine all box files into a single list\n    box_file_list = \" \".join(box_files)\n    \n    # Generate the unicharset file\n    os.system(f\"unicharset_extractor {box_file_list}\")\n    \n    # Create the font_properties file\n    with open(os.path.join(output_dir, \"font_properties\"), \"w\") as f:\n        f.write(\"Bangla 0 0 0 0 0\")\n    \n    # Generate the .tr files\n    for image_file, box_file in zip(image_files, box_files):\n        os.system(f\"tesseract {image_file} {image_file.replace('.png', '')} -l ben nobatch box.train\")\n    \n    # Generate the character set files\n    os.system(f\"mftraining -F {os.path.join(output_dir, 'font_properties')} -U unicharset -O {os.path.join(output_dir, 'output_unicharset')} {box_file_list}\")\n    os.system(f\"cntraining {box_file_list}\")\n    \n    # Combine the training data into a single traineddata file\n    os.system(f\"combine_tessdata {os.path.join(output_dir, 'ben.unicharset')}\")\n\n# Train Tesseract on the preprocessed images\ntrain_tesseract(preprocessed_image_files, box_files, \"/kaggle/working\")\n\n# Apply the trained model to perform OCR on the preprocessed images\nfor batch_idx in range(num_batches):\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, len(files))\n    \n    batch_filenames = files[start_idx:end_idx]\n    batch_extracted_texts = []\n    \n    # Loop through each file in the batch\n    for filename in batch_filenames:\n        # Construct file path for preprocessed image\n        file_path = os.path.join(preprocessed_dir, os.path.basename(filename))\n\n        # Open the preprocessed image\n        with Image.open(file_path) as preprocessed_image:\n            # Perform OCR on the preprocessed image using the custom trained model\n            extracted_text = pytesseract.image_to_string(preprocessed_image, lang='ben')\n\n        # Append filename and extracted text to lists\n        filenames.append(filename)\n        batch_extracted_texts.append(extracted_text)\n\n        # Display the image and extracted text\n        display(preprocessed_image)\n        print(f\"Extracted Text from {filename}:\\n{extracted_text}\\n\")\n    \n    # Add batch extracted texts to the main extracted texts list\n    extracted_texts.extend(batch_extracted_texts)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T06:54:09.480675Z","iopub.execute_input":"2024-05-31T06:54:09.481064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\nfrom IPython.display import display\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Set the batch size\nbatch_size = 50\nnum_batches = (len(files) + batch_size - 1) // batch_size\n\n# Function to resize an image to one-third of its original size\ndef resize_image(image_path):\n    with Image.open(image_path) as img:\n        original_width, original_height = img.size\n        target_width = original_width \n        target_height = original_height \n        img_resized = img.resize((target_width, target_height), Image.ANTIALIAS)\n    return img_resized\n\n# Loop through each batch of files\nfor batch_idx in range(num_batches):\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, len(files))\n    \n    batch_filenames = files[start_idx:end_idx]\n    batch_extracted_texts = []\n    \n    # Loop through each file in the batch\n    for filename in batch_filenames:\n        # Construct file path\n        file_path = os.path.join(source_dir, filename)\n\n        # Resize the image\n        resized_image = resize_image(file_path)\n\n        # Perform OCR on the resized image\n        extracted_text = pytesseract.image_to_string(resized_image)\n\n        # Append filename and extracted text to lists\n        filenames.append(filename)\n        batch_extracted_texts.append(extracted_text)\n\n        # Display the image and extracted text\n        display(resized_image)\n        print(f\"Extracted Text from {filename}:\\n{extracted_text}\\n\")\n    \n    # Add batch extracted texts to the main extracted texts list\n    extracted_texts.extend(batch_extracted_texts)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T07:12:23.816281Z","iopub.execute_input":"2024-05-31T07:12:23.816778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport pandas as pd\nfrom IPython.display import display\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Set the batch size\nbatch_size = 50\nnum_batches = (len(files) + batch_size - 1) // batch_size\n\n# Function to resize an image to one-third of its original size\ndef resize_image(image_path):\n    with Image.open(image_path) as img:\n        original_width, original_height = img.size\n        target_width = original_width // 3\n        target_height = original_height // 3\n        img_resized = img.resize((target_width, target_height), Image.ANTIALIAS)\n    return img_resized\n\n# Function to enhance image quality\ndef enhance_image(image):\n    # Apply contrast enhancement\n    enhancer = ImageEnhance.Contrast(image)\n    image = enhancer.enhance(2.0)  # Increase contrast by a factor of 2\n    \n    # Apply noise removal (Gaussian blur)\n    image = image.filter(ImageFilter.GaussianBlur(radius=1))\n    \n    return image\n\n# Loop through each batch of files\nfor batch_idx in range(num_batches):\n    start_idx = batch_idx * batch_size\n    end_idx = min((batch_idx + 1) * batch_size, len(files))\n    \n    batch_filenames = files[start_idx:end_idx]\n    batch_extracted_texts = []\n    \n    # Loop through each file in the batch\n    for filename in batch_filenames:\n        # Construct file path\n        file_path = os.path.join(source_dir, filename)\n\n        # Resize the image\n        resized_image = resize_image(file_path)\n        \n        # Enhance the image quality\n        enhanced_image = enhance_image(resized_image)\n\n        # Perform OCR on the enhanced image\n        extracted_text = pytesseract.image_to_string(enhanced_image)\n\n        # Append filename and extracted text to lists\n        filenames.append(filename)\n        batch_extracted_texts.append(extracted_text)\n\n        # Display the image and extracted text\n        display(enhanced_image)\n        print(f\"Extracted Text from {filename}:\\n{extracted_text}\\n\")\n    \n    # Add batch extracted texts to the main extracted texts list\n    extracted_texts.extend(batch_extracted_texts)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T04:20:33.606816Z","iopub.execute_input":"2024-05-31T04:20:33.607130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor filename in files[:10]:  # Process only the first 5 files\n    # Construct file path\n    file_path = os.path.join(source_dir, filename)\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Append filename and extracted text to lists\n    filenames.append(filename)\n    extracted_texts.append(extracted_text)\n\n    # Display the image and extracted text\n    display(image)\n    print(f\"Extracted Text from {filename}:\\n{extracted_text}\\n\")\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\noutput_dir = '/kaggle/working/renamed-images'     # New directory for renamed images\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor filename in files:\n    # Construct file paths\n    file_path = os.path.join(source_dir, filename)\n    output_file_path = os.path.join(output_dir, filename)\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Rename the file with the extracted text\n    new_filename = os.path.join(output_dir, extracted_text + '.jpg')  # Assuming it's a jpg file\n    shutil.copyfile(file_path, new_filename)\n\n    # Append extracted text to the list\n    extracted_texts.append(extracted_text)\n\n# Create a DataFrame from the extracted texts\ndata = {\"Original Filename\": files, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store renamed filenames and extracted texts\nrenamed_filenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor idx, filename in enumerate(files[:5], start=1):  # Process only the first 5 files\n    # Construct file path\n    file_path = os.path.join(source_dir, filename)\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Clean the extracted text\n    cleaned_text = ''.join(c for c in extracted_text if c.isprintable())\n\n    # Rename the filename\n    new_filename = f\"pres{idx}.jpg\"\n\n    # Append renamed filename and cleaned text to lists\n    renamed_filenames.append(new_filename)\n    extracted_texts.append(cleaned_text)\n\n    # Save the image with the new filename\n    new_file_path = os.path.join(source_dir, new_filename)\n    os.rename(file_path, new_file_path)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": renamed_filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Save the DataFrame to an Excel file\nexcel_file_path = '/kaggle/working/extracted_data.xlsx'  # Adjust the path as needed\ndf.to_excel(excel_file_path, index=False)\n\n# Display the path to the saved Excel file\nprint(f\"Excel file saved to: {excel_file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import difflib\n\ndef calculate_match_percentage(physician_name, extracted_text):\n    # Convert both strings to lowercase for better comparison\n    physician_name = physician_name.lower()\n    extracted_text = extracted_text.lower()\n\n    # Get the ratio of similarity between the physician name and the extracted text\n    similarity_ratio = difflib.SequenceMatcher(None, physician_name, extracted_text).ratio()\n\n    # Convert the ratio to a percentage\n    match_percentage = round(similarity_ratio * 100)\n\n    return match_percentage\n\n# Test data\ndata = [\n    {\"Filename\": \"PRS208C6014044.jpg\", \"Extracted Text\": \"oo weworoyrss aaapoy Â© SE PIIEA JONUs sy tyiedrey)Be\", \"PHY_NM\": \"DR. ANJAN BEPARY\", \"PHY_ID\": \"DHA25813\"},\n    {\"Filename\": \"PRS208C6020458.jpg\", \"Extracted Text\": \"Wis Gils It PET Seysanta, Fie, Cras), awa (Raa fb)Ie, Fy, er Cas cee coe aresorte dee eee Ser eet(cee, Btreie aetem, Bem)â€˜aaa Teco ves atest Creme cam)seh sete Cre, s,s eMSe fee worm Set, teDr. Md. Noor Kutubul AlamMBBS, BCS (Health) FCPS(EN.1)Ear, Nose Throat Spestals & Head-Neck SurgeonAdvanced Training in Endoscope Thytoid & Neck Surgery(Korea, Tata Cencer Hospital, Indi)Advanced! Training in Miro ear Surgery (France & Bangali)Assstant Profesor, Department of ENTJashore Medical College Hospital, ashoreBMDC No, A:39409 RS Mukter hossain 1D wai 32) wf; 08/04/202508047024510Phone: 018852648051 Cap. Esolok 20 mg .Chief Complaint Sens >t sean oieÂ© FBSENSATIONIN 2 Tab. Vifas 120 mgTHROAT rest oa rem en+ cough3 Tab. Deflux 10 mgiagnosi Se yet saa toraagnosis :+ CHRONIC PHARYNGITIS: 4. RHINOMIST NASAL SPRAYR40raca cl tea Signature\", \"PHY_NM\": \"DR. MD. ZAHANGIR ALAM (BABU)\", \"PHY_ID\": \"DHA14913\"},\n    # Add more data entries as needed\n]\n\n# Calculate match percentage for each entry\nfor entry in data:\n    match_percentage = calculate_match_percentage(entry[\"PHY_NM\"], entry[\"Extracted Text\"])\n    entry[\"Match_Percentage\"] = match_percentage\n\n# Print the results\nprint(\"Filename\\tExtracted Text\\tPHY_NM\\tPHY_ID\\tMatch_Percentage\")\nfor entry in data:\n    print(f\"{entry['Filename']}\\t{entry['Extracted Text']}\\t{entry['PHY_NM']}\\t{entry['PHY_ID']}\\t{entry['Match_Percentage']}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# Output directory for renamed files\noutput_dir = '/kaggle/output'  # Adjust the path accordingly\n\n# Create the output directory if it does not exist\nos.makedirs(output_dir, exist_ok=True)\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store renamed filenames and extracted texts\nrenamed_filenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor idx, filename in enumerate(files[:100], start=1):  # Process only the first 5 files\n    # Construct file paths\n    file_path = os.path.join(source_dir, filename)\n    new_file_path = os.path.join(output_dir, f\"pres{idx}.jpg\")\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image with fine-tuning\n    extracted_text = pytesseract.image_to_string(image, config='--oem 1 --psm 6')\n\n    # Clean the extracted text\n    cleaned_text = ''.join(c for c in extracted_text if c.isprintable())\n\n    # Append renamed filename and cleaned text to lists\n    renamed_filenames.append(new_file_path)\n    extracted_texts.append(cleaned_text)\n\n    # Save the image with the new filename\n    image.save(new_file_path)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": renamed_filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Save the DataFrame to an Excel file\nexcel_file_path = '/kaggle/working/extracted_data.xlsx'  # Adjust the path as needed\ndf.to_excel(excel_file_path, index=False)\n\n# Display the path to the saved Excel file\nprint(f\"Excel file saved to: {excel_file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store filenames and extracted texts\nfilenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor filename in files[:309]:  # Process only the first 5 files\n    # Construct file path\n    file_path = os.path.join(source_dir, filename)\n\n    # Open the image file\n    image = Image.open(file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Clean the extracted text\n    cleaned_text = ''.join(c for c in extracted_text if c.isprintable())\n\n    # Append filename and cleaned text to lists\n    filenames.append(filename)\n    extracted_texts.append(cleaned_text)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Save the DataFrame to an Excel file\nexcel_file_path = '/kaggle/working/extracted_data.xlsx'  # Adjust the path as needed\ndf.to_excel(excel_file_path, index=False)\n\n# Display the path to the saved Excel file\nprint(f\"Excel file saved to: {excel_file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport shutil\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n# Ensure that Tesseract OCR is installed\n!sudo apt-get install tesseract-ocr\n\n# Install pytesseract\n!pip install pytesseract\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# New directory for renamed files\nrenamed_dir = '/kaggle/working/renamed_images'\nos.makedirs(renamed_dir, exist_ok=True)\n\n# List all files in the source directory\nfiles = os.listdir(source_dir)\n\n# Initialize lists to store renamed filenames and extracted texts\nrenamed_filenames = []\nextracted_texts = []\n\n# Loop through each file in the source directory\nfor idx, filename in enumerate(files[:309], start=1):  # Process only the first 309 files\n    # Construct file paths\n    file_path = os.path.join(source_dir, filename)\n    new_filename = f\"pres{idx}.jpg\"\n    new_file_path = os.path.join(renamed_dir, new_filename)\n\n    # Copy the image file with the new filename\n    shutil.copyfile(file_path, new_file_path)\n\n    # Open the image file\n    image = Image.open(new_file_path)\n\n    # Perform OCR on the image\n    extracted_text = pytesseract.image_to_string(image)\n\n    # Clean the extracted text\n    cleaned_text = ''.join(c for c in extracted_text if c.isprintable())\n\n    # Append renamed filename and cleaned text to lists\n    renamed_filenames.append(new_filename)\n    extracted_texts.append(cleaned_text)\n\n# Create a DataFrame from the lists\ndata = {\"Filename\": renamed_filenames, \"Extracted Text\": extracted_texts}\ndf = pd.DataFrame(data)\n\n# Save the DataFrame to an Excel file\nexcel_file_path = '/kaggle/working/extracted_data.xlsx'  # Adjust the path as needed\ndf.to_excel(excel_file_path, index=False)\n\n# Display the path to the saved Excel file\nprint(f\"Excel file saved to: {excel_file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nfrom PIL import Image\n\n# Source directory\nsource_dir = '/kaggle/input/prescription-image'  # Adjust the path accordingly\n\n# Output directory for training data\ntraining_data_dir = '/kaggle/working/training_data'  # Adjust the path as needed\n\n# Create the output directory if it doesn't exist\nos.makedirs(training_data_dir, exist_ok=True)\n\n# Function to generate training data files\ndef generate_training_data(image_file, text):\n    base_name = os.path.splitext(os.path.basename(image_file))[0]\n    with open(os.path.join(training_data_dir, f\"{base_name}.gt.txt\"), 'w') as f:\n        f.write(text)\n    # Tesseract requires TIFF format for training data\n    image = Image.open(image_file)\n    image.save(os.path.join(training_data_dir, f\"{base_name}.tif\"))\n\n# Loop through each file in the source directory\nfor filename in os.listdir(source_dir):\n    file_path = os.path.join(source_dir, filename)\n    # Here you need to extract the ground truth text for each image\n    # You can manually annotate the text or use an existing dataset with annotations\n    ground_truth_text = \"Dr. Asit BaranPGT (Ortho), D-orthe (Course)9 arg, wo, sre| eres 9 Grope mrers wD Medical Officercarn, Gan wratet sm mea 5| ret corre Bors ES r       t MetivbboyKeod sf RestAeacderd\\ ¢  Foe we er oe rae seecontaSHIP o\"\n    generate_training_data(file_path, ground_truth_text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Load images\nimage_files = [\n    '/mnt/data/PRS208C4002621.jpg',\n    '/mnt/data/PRS208C4017521.jpg',\n    '/mnt/data/PRS208C4018544.jpg',\n    '/mnt/data/PRS208C4018582.jpg',\n    '/mnt/data/PRS208C4018608.jpg',\n    '/mnt/data/PRS208C4018624.jpg',\n    '/mnt/data/PRS208C4018627.jpg'\n]\n\nprocessed_images = []\n\nfor file in image_files:\n    # Read the image\n    image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n    \n    # Resize the image\n    resized_image = cv2.resize(image, (128, 32))\n    \n    # Binarize the image\n    _, binary_image = cv2.threshold(resized_image, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Append processed image\n    processed_images.append(binary_image)\n\n# Display the first processed image as an example\nplt.imshow(processed_images[0], cmap='gray')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}